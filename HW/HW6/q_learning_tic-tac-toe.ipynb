{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q learning for tic-tac-toe\n",
    "\n",
    "This was adapted from:\n",
    "https://towardsdatascience.com/reinforcement-learning-implement-tictactoe-189582bea542"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [02:06<00:00, 792.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio p1 wins: 45.32%\n",
      "Ratio p2 wins: 17.07%\n",
      "Ratio ties: 37.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 3\n",
    "\n",
    "class State:\n",
    "    def __init__(self, p1, p2):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.isEnd = False\n",
    "        self.boardHash = None\n",
    "        # init p1 plays first\n",
    "        self.playerSymbol = 1\n",
    "    \n",
    "    # get unique hash of current board state\n",
    "    def getHash(self):\n",
    "        self.boardHash = str(self.board.reshape(BOARD_COLS*BOARD_ROWS))\n",
    "        return self.boardHash\n",
    "    \n",
    "    def winner(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            1 if p1 wins\n",
    "            -1 if p2 wins\n",
    "            0 if draw\n",
    "            None if not end yet\n",
    "        '''\n",
    "        # row\n",
    "        for i in range(BOARD_ROWS):\n",
    "            if sum(self.board[i, :]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[i, :]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "        # col\n",
    "        for i in range(BOARD_COLS):\n",
    "            if sum(self.board[:, i]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[:, i]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "        # diagonal\n",
    "        diag_sum1 = sum([self.board[i, i] for i in range(BOARD_COLS)])\n",
    "        diag_sum2 = sum([self.board[i, BOARD_COLS-i-1] for i in range(BOARD_COLS)])\n",
    "        diag_sum = max(diag_sum1, diag_sum2)\n",
    "        if diag_sum == 3:\n",
    "            self.isEnd = True\n",
    "            return 1\n",
    "        if diag_sum == -3:\n",
    "            self.isEnd = True\n",
    "            return -1\n",
    "        \n",
    "        # tie\n",
    "        # no available positions\n",
    "        if len(self.availablePositions()) == 0:\n",
    "            self.isEnd = True\n",
    "            return 0\n",
    "\n",
    "        # not end\n",
    "        self.isEnd = False\n",
    "        return None\n",
    "    \n",
    "    def availablePositions(self):\n",
    "        positions = []\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                if self.board[i, j] == 0:\n",
    "                    positions.append((i, j))  # need to be tuple\n",
    "        return positions\n",
    "    \n",
    "    def updateState(self, position):\n",
    "        self.board[position] = self.playerSymbol\n",
    "        # switch to another player\n",
    "        self.playerSymbol = -1 if self.playerSymbol == 1 else 1\n",
    "    \n",
    "    # only when game ends\n",
    "    def giveReward(self):\n",
    "        result = self.winner()\n",
    "        # backpropagate reward\n",
    "        if result == 1:\n",
    "            self.p1.feedReward(1)\n",
    "            self.p2.feedReward(0)\n",
    "        elif result == -1:\n",
    "            self.p1.feedReward(0)\n",
    "            self.p2.feedReward(1)\n",
    "        else:\n",
    "            self.p1.feedReward(0.1)\n",
    "            self.p2.feedReward(0.5)\n",
    "    \n",
    "    # board reset\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.boardHash = None\n",
    "        self.isEnd = False\n",
    "        self.playerSymbol = 1\n",
    "    \n",
    "    def play_against_computer(self, rounds=100, train=True):\n",
    "        wins = []\n",
    "\n",
    "        for i in tqdm(range(rounds)):\n",
    "            while not self.isEnd:\n",
    "                # Player 1\n",
    "                positions = self.availablePositions()\n",
    "                p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
    "                # take action and upate board state\n",
    "                self.updateState(p1_action)\n",
    "                board_hash = self.getHash()\n",
    "                self.p1.addState(board_hash)\n",
    "                # check board status if it is end\n",
    "\n",
    "                win = self.winner()\n",
    "                if win is not None:\n",
    "                    wins.append(win)\n",
    "\n",
    "                    # ended with p1 either win or draw\n",
    "                    if train:\n",
    "                        self.giveReward()\n",
    "                    self.p1.reset()\n",
    "                    self.p2.reset()\n",
    "                    self.reset()\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    # Player 2\n",
    "                    positions = self.availablePositions()\n",
    "                    p2_action = self.p2.chooseAction(positions, self.board, self.playerSymbol)\n",
    "                    self.updateState(p2_action)\n",
    "                    board_hash = self.getHash()\n",
    "                    self.p2.addState(board_hash)\n",
    "                    \n",
    "                    win = self.winner()\n",
    "                    if win is not None:\n",
    "                        wins.append(win)\n",
    "\n",
    "                        # ended with p2 either win or draw\n",
    "                        if train:\n",
    "                            self.giveReward()\n",
    "                        self.p1.reset()\n",
    "                        self.p2.reset()\n",
    "                        self.reset()\n",
    "                        break\n",
    "        \n",
    "        print(f\"Ratio p1 wins: {wins.count(1)/rounds*100:.2f}%\")\n",
    "        print(f\"Ratio p2 wins: {wins.count(-1)/rounds*100:.2f}%\")\n",
    "        print(f\"Ratio ties: {wins.count(0)/rounds*100:.2f}%\")\n",
    "    \n",
    "    # play with human\n",
    "    def play_against_human(self):\n",
    "        while not self.isEnd:\n",
    "            # Player 1\n",
    "            positions = self.availablePositions()\n",
    "            p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
    "            # take action and upate board state\n",
    "            self.updateState(p1_action)\n",
    "            self.showBoard()\n",
    "            # check board status if it is end\n",
    "            win = self.winner()\n",
    "            if win is not None:\n",
    "                if win == 1:\n",
    "                    print(self.p1.name, \"wins!\")\n",
    "                else:\n",
    "                    print(\"tie!\")\n",
    "                self.reset()\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                # Player 2\n",
    "                positions = self.availablePositions()\n",
    "                p2_action = self.p2.chooseAction(positions)\n",
    "\n",
    "                self.updateState(p2_action)\n",
    "                self.showBoard()\n",
    "                win = self.winner()\n",
    "                if win is not None:\n",
    "                    if win == -1:\n",
    "                        print(self.p2.name, \"wins!\")\n",
    "                    else:\n",
    "                        print(\"tie!\")\n",
    "                    self.reset()\n",
    "                    break\n",
    "\n",
    "    def showBoard(self):\n",
    "        # p1: x  p2: o\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-------------', flush=True)\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out, flush=True)\n",
    "        print('-------------', flush=True)    \n",
    "\n",
    "class Player:\n",
    "    def __init__(self, name, exp_rate=0.3):\n",
    "        self.name = name\n",
    "        self.states = []  # record all positions taken\n",
    "        self.lr = 0.2\n",
    "        self.exp_rate = exp_rate\n",
    "        self.decay_gamma = 0.9\n",
    "        self.states_value = {}  # state -> value\n",
    "    \n",
    "    def getHash(self, board):\n",
    "        boardHash = str(board.reshape(BOARD_COLS*BOARD_ROWS))\n",
    "        return boardHash\n",
    "    \n",
    "    def chooseAction(self, positions, current_board, symbol):\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            # take random action\n",
    "            idx = np.random.choice(len(positions))\n",
    "            action = positions[idx]\n",
    "        else:\n",
    "            value_max = -999\n",
    "            for p in positions:\n",
    "                next_board = current_board.copy()\n",
    "                next_board[p] = symbol\n",
    "                next_boardHash = self.getHash(next_board)\n",
    "                value = 0 if self.states_value.get(next_boardHash) is None else self.states_value.get(next_boardHash)\n",
    "                if value >= value_max:\n",
    "                    value_max = value\n",
    "                    action = p\n",
    "        return action\n",
    "    \n",
    "    # append a hash state\n",
    "    def addState(self, state):\n",
    "        self.states.append(state)\n",
    "    \n",
    "    # at the end of game, backpropagate and update states value\n",
    "    def feedReward(self, reward):\n",
    "        for st in reversed(self.states):\n",
    "            if self.states_value.get(st) is None:\n",
    "                self.states_value[st] = 0\n",
    "            self.states_value[st] += self.lr*(self.decay_gamma*reward - self.states_value[st])\n",
    "            reward = self.states_value[st]\n",
    "            \n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        \n",
    "    def savePolicy(self):\n",
    "        fw = open('policy_' + str(self.name), 'wb')\n",
    "        pickle.dump(self.states_value, fw)\n",
    "        fw.close()\n",
    "\n",
    "    def loadPolicy(self, file):\n",
    "        fr = open(file,'rb')\n",
    "        self.states_value = pickle.load(fr)\n",
    "        fr.close()\n",
    "\n",
    "p1 = Player(\"p1\", exp_rate=0.3)\n",
    "p2 = Player(\"p2\", exp_rate=0.3)\n",
    "\n",
    "st = State(p1, p2)\n",
    "print(\"training...\")\n",
    "st.play_against_computer(100000)\n",
    "\n",
    "p1.savePolicy()\n",
    "p2.savePolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "https://blog.ostermiller.org/tic-tac-toe-strategy/\n",
    "\n",
    "To evaluate how well our agent is doing we can evaluate it against a completely random player. According to \n",
    "Ostermiller's blog, we can if the agent has become an expert and plays completely optimal moves then the \n",
    "rates are the following assuming P1 goes first:\n",
    "\n",
    "P1 (Expert), P2 (Random):\n",
    "- P1 wins: 97.8%\n",
    "- P2 wins: 0.00%\n",
    "- Ties: 2.20%\n",
    "\n",
    "P1 (Random), P2 (Expert):\n",
    "- P1 wins: 0.00%\n",
    "- P2 wins: 79.6%\n",
    "- Ties: 20.4%\n",
    "\n",
    "We can run and evaulation and compare the ratios to see if our agent learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1073.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio p1 wins: 98.98%\n",
      "Ratio p2 wins: 0.00%\n",
      "Ratio ties: 1.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p1 = Player(\"computer\", exp_rate=0)\n",
    "p1.loadPolicy(\"policy_p1\")\n",
    "p2 = Player(\"random\", exp_rate=1)\n",
    "\n",
    "st = State(p1, p2)\n",
    "print(\"playing...\")\n",
    "st.play_against_computer(10000, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1044.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio p1 wins: 0.10%\n",
      "Ratio p2 wins: 57.76%\n",
      "Ratio ties: 42.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p1 = Player(\"random\", exp_rate=1)\n",
    "p2 = Player(\"computer\", exp_rate=0)\n",
    "p2.loadPolicy(\"policy_p2\")\n",
    "\n",
    "st = State(p1, p2)\n",
    "print(\"playing...\")\n",
    "st.play_against_computer(10000, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our P1 agent learned the optimal strategy to play because the ratio's match up. However, the P2 agent's ratio does not match. This could be because P1's agent learned the optimal strategy quicker than P2's and thus it couldn't learn anymore.\n",
    "\n",
    "# Human examples\n",
    "Here are some examples with a human player:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row and Col numbers are from 1 to 3.\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "-------------\n",
      "|   |   | o | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "-------------\n",
      "|   | x | o | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "-------------\n",
      "|   | x | o | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   | o |   | \n",
      "-------------\n",
      "-------------\n",
      "|   | x | o | \n",
      "-------------\n",
      "|   | x | x | \n",
      "-------------\n",
      "|   | o |   | \n",
      "-------------\n",
      "-------------\n",
      "|   | x | o | \n",
      "-------------\n",
      "| o | x | x | \n",
      "-------------\n",
      "|   | o |   | \n",
      "-------------\n",
      "-------------\n",
      "| x | x | o | \n",
      "-------------\n",
      "| o | x | x | \n",
      "-------------\n",
      "|   | o |   | \n",
      "-------------\n",
      "-------------\n",
      "| x | x | o | \n",
      "-------------\n",
      "| o | x | x | \n",
      "-------------\n",
      "|   | o | o | \n",
      "-------------\n",
      "-------------\n",
      "| x | x | o | \n",
      "-------------\n",
      "| o | x | x | \n",
      "-------------\n",
      "| x | o | o | \n",
      "-------------\n",
      "tie!\n"
     ]
    }
   ],
   "source": [
    "class HumanPlayer:\n",
    "    def __init__(self, name):\n",
    "        self.name = name \n",
    "    \n",
    "    def chooseAction(self, positions):\n",
    "        while True:\n",
    "            row = int(input(\"Input your action row:\"))-1\n",
    "            col = int(input(\"Input your action col:\"))-1\n",
    "            action = (row, col)\n",
    "            if action in positions:\n",
    "                return action\n",
    "    \n",
    "    # append a hash state\n",
    "    def addState(self, state):\n",
    "        pass\n",
    "    \n",
    "    # at the end of game, backpropagate and update states value\n",
    "    def feedReward(self, reward):\n",
    "        pass\n",
    "            \n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "p1 = Player(\"computer\", exp_rate=0)\n",
    "p1.loadPolicy(\"policy_p1\")\n",
    "\n",
    "p2 = HumanPlayer(\"human\")\n",
    "\n",
    "st = State(p1, p2)\n",
    "print(\"Row and Col numbers are from 1 to 3.\")\n",
    "st.play_against_human()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row and Col numbers are from 1 to 3.\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   |   | o | \n",
      "-------------\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   | x | o | \n",
      "-------------\n",
      "-------------\n",
      "|   | o |   | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   | x | o | \n",
      "-------------\n",
      "-------------\n",
      "|   | o |   | \n",
      "-------------\n",
      "| x | x |   | \n",
      "-------------\n",
      "|   | x | o | \n",
      "-------------\n",
      "-------------\n",
      "|   | o |   | \n",
      "-------------\n",
      "| x | x | o | \n",
      "-------------\n",
      "|   | x | o | \n",
      "-------------\n",
      "-------------\n",
      "|   | o | x | \n",
      "-------------\n",
      "| x | x | o | \n",
      "-------------\n",
      "|   | x | o | \n",
      "-------------\n",
      "-------------\n",
      "|   | o | x | \n",
      "-------------\n",
      "| x | x | o | \n",
      "-------------\n",
      "| o | x | o | \n",
      "-------------\n",
      "-------------\n",
      "| x | o | x | \n",
      "-------------\n",
      "| x | x | o | \n",
      "-------------\n",
      "| o | x | o | \n",
      "-------------\n",
      "tie!\n"
     ]
    }
   ],
   "source": [
    "print(\"Row and Col numbers are from 1 to 3.\")\n",
    "st.play_against_human()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row and Col numbers are from 1 to 3.\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "-------------\n",
      "|   |   | o | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "-------------\n",
      "|   | x | o | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "-------------\n",
      "| o | x | o | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "-------------\n",
      "| o | x | o | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "computer wins!\n"
     ]
    }
   ],
   "source": [
    "print(\"Row and Col numbers are from 1 to 3.\")\n",
    "st.play_against_human()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
